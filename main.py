# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GGr5rRNggNQY6TXZhxj3eqUm08olnDap
"""

import requests
import asyncio
import random
import pytz
import json
import os
from datetime import datetime
from bs4 import BeautifulSoup
from requests.exceptions import RequestException
from openai import OpenAI
from dotenv import load_dotenv
from telegram import Bot

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
TOKEN = os.getenv("TELEGRAM_TOKEN")
CHAT_ID = os.getenv("CHAT_ID")
tz_bogota = pytz.timezone('America/Bogota')
today_bogota = datetime.now(tz_bogota).date()

BASE_URL = "https://www.revistapym.com.co/"
INDEX_PATHS = [
    '/articulos/comunicacion',
    '/articulos/mercadeo',
    '/articulos/consumidor',
    '/articulos/marcas-sostenibles',
    '/articulos/digital',
    '/articulos/gente',
]
ROUTES_FILE = r'Routes.json'
NEWS_FILE = r'News.json'
REQUESTS_TIMEOUT = 10
SLEEP_BETWEEN_REQUESTS = 2.0
MONTHS = {
    "enero": 1, "febrero": 2, "marzo": 3, "abril": 4,
    "mayo": 5, "junio": 6, "julio": 7, "agosto": 8,
    "septiembre": 9, "octubre": 10, "noviembre": 11, "diciembre": 12
}

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:56.0) Gecko/20100101 Firefox/56.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36",
]

headers = {
    "User-Agent": random.choice(USER_AGENTS),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",  # Indicamos que aceptamos respuestas comprimidas
    "Accept-Language": "es-ES,es;q=0.9,en;q=0.8",  # Idiomas preferidos
    "Connection": "keep-alive",  # Mantener la conexi√≥n abierta
    "Upgrade-Insecure-Requests": "1",  # Soporta peticiones de actualizaci√≥n
    "Referer": BASE_URL,  # Referencia al sitio origen
    "TE": "Trailers",  # Protocolo HTTP/2 para recursos
}

def get_session(user_agent: str = None, timeout: int = 10) -> requests.Session:
    s = requests.Session()

    # Si no se pasa un User-Agent, elegir uno aleatorio de la lista
    user_agent = user_agent or random.choice(USER_AGENTS)

    # Headers m√°s completos
    s.headers.update({
        "User-Agent": user_agent,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Accept-Language": "es-ES,es;q=0.9,en;q=0.8",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Referer": "https://www.revistapym.com.co",  # Cambia seg√∫n la web de destino
        "TE": "Trailers",
    })

    # Establecer un timeout por defecto
    s.timeout = timeout

    return s

def get_page_content(url: str, session: requests.Session, retries: int = 3) -> str:
    """
    Obtiene el contenido HTML de una p√°gina a partir de la URL dada.

    :param url: La URL a la que hacer la solicitud GET.
    :param session: La sesi√≥n de requests con los headers configurados.
    :param retries: N√∫mero de reintentos en caso de error.
    :return: El contenido HTML de la p√°gina o un mensaje de error si no se pudo obtener.
    """
    try:
        # Intentar obtener el contenido con un n√∫mero de reintentos
        response = session.get(url, timeout=REQUESTS_TIMEOUT)
        response.raise_for_status()  # Lanza un error si el c√≥digo de respuesta no es 200
        return response.text  # Retorna el HTML de la p√°gina

    except RequestException as e:
        # Si ocurre alg√∫n error, como problemas de conexi√≥n, timeout, etc.
        if retries > 0:
            print(f"Error al obtener la p√°gina. Reintentando... ({retries} intentos restantes)")
            return get_page_content(url, session, retries - 1)  # Reintentar
        else:
            print(f"Error al obtener la p√°gina: {e}")
            return f"Error: {e}"

def get_links_from_page(url: str):
  if os.path.exists(ROUTES_FILE):
    with open(ROUTES_FILE, 'r', encoding='utf-8') as file:
      try:
        saved_links = json.load(file)
      except json.JSONDecodeError:
        saved_links = []
  else:
    saved_links = []

  session = requests.Session()
  session.headers.update(headers)

  soup = BeautifulSoup(get_page_content(url,session), "html.parser")

  found_links = set()

  main_new = soup.find('div', class_="opening container").find('a', href=True)
  href = main_new['href'].strip()
  if href and not href.startswith('#'):
      href = BASE_URL.rstrip('/') + href
  found_links.add(href)

  list_news = soup.find('div', class_=["list-news", "list-news-1"])

  for a in list_news.find_all('a', href=True):
      href = a['href'].strip()

      if href and not href.startswith('#'):
          href = BASE_URL.rstrip('/') + href
          found_links.add(href)
      else:
        None

  new_links = [link for link in found_links if link not in saved_links and link != BASE_URL]

  if new_links:
    saved_links.extend(new_links)
    with open(ROUTES_FILE, 'w', encoding='utf-8') as file:
      json.dump(saved_links, file, ensure_ascii=False, indent=2)
    print(f"‚úÖ Se agregaron {len(new_links)} nuevos enlaces al archivo {ROUTES_FILE} provenientes de {url.rstrip('/')}")
  else:
      print("‚ÑπÔ∏è No se encontraron nuevos enlaces para agregar.")

# Execution

try:
  for path in INDEX_PATHS:
    get_links_from_page(BASE_URL.rstrip('/') + path)
except Exception as e:
  print(e)

def get_content_from_new():
  with open(ROUTES_FILE, 'r', encoding='utf-8') as file:
    link_list = json.load(file)

  session = requests.Session()
  session.headers.update(headers)

  news = []

  for i, link in enumerate(link_list):
    new = {}
    soup = BeautifulSoup(get_page_content(link, session), "html.parser")

    title = soup.find('h1').text.strip()
    date = soup.find('span', class_="date").text.strip()
    description = soup.find('div', class_="content-lead").find('p').text.strip()
    content = ' '.join(
        p.get_text(strip=True)
        for p in soup.find('div', class_="body-content post-wrap").find_all('p')
    )

    try:
        month, day, year = date.split()
        month = MONTHS[month]
        day = int(day.replace(",", ""))
        year = int(year)
        date_obj = datetime(year, month, day).date()
    except Exception as e:
        print(f"‚ö†Ô∏è No se pudo interpretar la fecha '{date}' ‚Üí {e}")
        continue

    new['link'] = link
    new['date'] = date
    new['title'] = title
    new['description'] = description
    new['content'] = content
    news.append(new)

  if news:
    with open(NEWS_FILE, 'w', encoding='utf-8') as file:
      json.dump(news, file, ensure_ascii=False, indent=2)
      print(f"‚úÖ Se agregaron {len(news)} nuevos enlaces al archivo {NEWS_FILE}")
  else:
      print("‚ÑπÔ∏è No se encontraron nuevos enlaces para agregar.")

get_content_from_new()

def get_summary_fron_ChatGPT(new:dict):

  try:
      month, day, year = new['date'].split()
      month = MONTHS[month]
      day = int(day.replace(",", ""))
      year = int(year)
      date = datetime(year, month, day).date()
  except Exception as e:
      print(f"‚ö†Ô∏è No se pudo interpretar la fecha '{date}' ‚Üí {e}")

  max_chars = 3200
  prompt = f"Summarize the following news article in Spanish as a single HTML string suitable for Telegram (parse_mode='HTML') with a maximum length of {max_chars} characters: start with the article title wrapped in <b>...</b> and add two relevant emojis the second show the new's importance or impact (color circle: red, orange, blue, yellow and green where red is most important and green least importante), then a concise, coherent summary that captures main facts, context and relevance using <b>bold</b> for key terms and <i>italics</i> for nuances, avoid any Markdown or unsupported tags, and finish with a clickable source link like üîó <a href=\"{new['link']}\">Fuente</a>; output must be valid Telegram HTML and must not exceed {max_chars} characters."

  new_msg = prompt + '\n\n' + new['link'] + new['title'] + '\n' + new['description']

  if date == today_bogota:
    response = client.chat.completions.create(
        model = "gpt-4o-mini",
        messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": new_msg}
      ]
    )
  else:
    print(f"‚è≠Ô∏è Noticia ignorada: {title} ({date})")

  reply = response.choices[0].message.content
  return reply

def fix_markdown(text:str):
  markdown_chars = r"_*[]()~`>#+-=|{}.!"
  for ch in markdown_chars:
    text = text.replace(ch, "\\" + ch)
  return text

async def send_summary_to_telegram():
  with open(NEWS_FILE, 'r', encoding='utf-8') as file:
    news = json.load(file)

  bot = Bot(token=TOKEN)

  today_news = 0
  index = []

  for i, new in enumerate(news):
    try:
        month, day, year = new['date'].split()
        month = MONTHS[month]
        day = int(day.replace(",", ""))
        year = int(year)
        date = datetime(year, month, day).date()

        if date == today_bogota:
          today_news += 1
          index.append(i)
    except Exception as e:
        continue

  for j, i in enumerate(index):
    new = news[i]
    try:
        month, day, year = new['date'].split()
        month = MONTHS[month]
        day = int(day.replace(",", ""))
        year = int(year)
        date = datetime(year, month, day).date()

        if date == today_bogota:
          msg = get_summary_fron_ChatGPT(new)
          print(f"Noticia {j+1}/{today_news}: {new['date']} | {new['title']}")
          await bot.send_message(chat_id=CHAT_ID, text=msg, parse_mode="HTML")

    except Exception as e:
        print(f"‚ö†Ô∏è No se pudo interpretar la fecha '{date}' ‚Üí {e}")
        continue

asyncio.run(send_summary_to_telegram())